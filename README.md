# books-data-pipeline
Python ETL pipeline that scrapes 1000+ books and loads them into a MySQL database.
# ğŸ“š Books Data Pipeline

This project demonstrates how to build a mini ETL (Extract, Transform, Load) pipeline in Python by scraping real-world book data and storing it in a MySQL relational database.

## ğŸ”§ What This Project Does

- ğŸ•¸ Scrapes 1000+ books from [books.toscrape.com](http://books.toscrape.com/)
- ğŸ“„ Saves data to a clean CSV file
- ğŸ›¢ Creates a structured MySQL database with two tables: `categories` and `books`
- ğŸ§  Loads and stores scraped data into the database using Python

## ğŸ—‚ Tech Stack

- Python (BeautifulSoup, requests, pandas, mysql-connector)
- MySQL
- Jupyter Notebook
- Git & GitHub

## ğŸ“ Project Structure

books-data-pipeline/
â”‚
â”œâ”€â”€ data/ # CSV scraped data
â”œâ”€â”€ scripts/ # Python scripts for scraping and database loading
â”œâ”€â”€ schema/ # ERD diagram and schema visuals
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ requirements.txt # Python dependencies

## ğŸš€ How to Run

1. Clone the repo  
2. Install requirements: `pip install -r requirements.txt`  
3. Run `web_scraper.py` to extract data  
4. Run `db_loader.py` to create tables and load into MySQL  

## ğŸ§  What I Learned

- Web scraping with BeautifulSoup
- Structuring data with relational models
- Writing SQL in Python
- Basic ETL design and implementation
